{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "\n",
    "The objective of this session is to implement a multi-layer perceptron with one hidden layer from scratch and test it on MNIST.\n",
    "You can get information about the practical sessions and the provided helper functions on the course’s website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Activation function\n",
    "\n",
    "Write the two functions\n",
    "    \n",
    "    def sigma(x)\n",
    "    def dsigma(x)\n",
    " \n",
    "that take as input a float tensor and returns a tensor of same size, obtained by applying component-wise respectively tanh, and the first derivative of tanh.\n",
    "\n",
    "Hint: The functions should have no python loop, and use in particular torch.tanh , torch.exp , torch.mul , and torch.pow . My versions are 34 and 62 character long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "import dlc_practical_prologue as prologue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma(x):\n",
    "    return torch.tanh(x)\n",
    "\n",
    "def dsigma(x):\n",
    "    tanX2 = torch.tanh(x).pow(2)\n",
    "    return 1 - tanX2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2131524., 12771452.],\n",
      "        [ 6708555., 13028076.]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[0., 0.],\n",
      "        [0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor(2,2).random_()\n",
    "print(x)\n",
    "print(sigma(x))\n",
    "print(dsigma(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma(x):\n",
    "    return x.tanh()\n",
    "\n",
    "def dsigma(x):\n",
    "    return 4 * (x.exp() + x.mul(-1).exp()).pow(-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[0., 0.],\n",
      "        [0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print(sigma(x))\n",
    "print(dsigma(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note :\n",
    "Pas de faute, mais faire gaffe à bien se rappeler formule tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Loss\n",
    "\n",
    "Write the two functions\n",
    "\n",
    "    def loss(v, t)\n",
    "    def dloss(v, t)\n",
    "\n",
    "that take as input two float tensors of same dimensions with v the predicted tensor and t the target one, and return respectively ∥t − v ∥2 , and a tensor of same size equal to the gradient of that quantity as a function of v.\n",
    "\n",
    "Hint: The functions should have no python loop, and use in particular torch.sum , torch.pow . My versions are 48 and 40 character long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(v, t):\n",
    "    return (t-v).pow(2).sum()\n",
    "\n",
    "def dloss(v, t):\n",
    "    ret = (t-v)*loss(v,t).sum(0)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(v, t):\n",
    "    return (v - t).pow(2).sum()\n",
    "\n",
    "def dloss(v, t):\n",
    "    return 2 * (v - t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "\n",
    "Revoir sa définition du gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Forward and backward passes\n",
    "\n",
    "Write a function\n",
    "\n",
    "    def forward ̇pass(w1, b1, w2, b2, x)\n",
    "\n",
    "whose arguments correspond to an input vector to the network, and the weight and bias of the two layers, and returns a tuple composed of the corresponding x(0), s(1), x(1), s(2), and x(2).\n",
    "\n",
    "Write a function\n",
    "    \n",
    "    def backward ̇pass(w1, b1, w2, b2,\n",
    "                  t,\n",
    "                  x, s1, x1, s2, x2,\n",
    "                  dl ̇dw1, dl ̇db1, dl ̇dw2, dl ̇db2)\n",
    "\n",
    "whose arguments correspond to the target vector, the quantities computed by the forward pass, and the tensors used to store the cumulated sums of the gradient on individual samples, and update the latters according to the formula of the backward pass.\n",
    "\n",
    "Hint: The functions should have no python loop, and use in particular torch.t, torch.mv, torch.mm , and torch.view , and the functions previously written. The main difficulty is to deal\n",
    "properly with the tensor size and transpose. My versions are 165 and 436 character long.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(w1, b1, w2, b2, x):\n",
    "    s1 = torch.mm(w1, x) + b1\n",
    "    x1 = sigma(s1)\n",
    "    s2 = torch.mm(w2, x1) + b2\n",
    "    x2 = sigma(s2)\n",
    "    return x, s1, x1, s2, x2\n",
    "    \n",
    "def backward_pass(w1, b1, w2, b2, t, x, s1, x1, s2, x2, d1_dw1, d1_db1, d1_dw2, d1_db2):\n",
    "    w1 = w1 - t*d1_dw1\n",
    "    w2 = w2 - t*d1_dw2\n",
    "    b1 = b1 - t*d1_db1\n",
    "    d2 = b2 - t*d1_db2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(w1, b1, w2, b2, x):\n",
    "    x0 = x\n",
    "    s1 = w1.mv(x0) + b1\n",
    "    x1 = sigma(s1)\n",
    "    s2 = w2.mv(x1) + b2\n",
    "    x2 = sigma(s2)\n",
    "\n",
    "    return x0, s1, x1, s2, x2\n",
    "\n",
    "def backward_pass(w1, b1, w2, b2,\n",
    "                  t,\n",
    "                  x, s1, x1, s2, x2,\n",
    "                  dl_dw1, dl_db1, dl_dw2, dl_db2):\n",
    "    x0 = x\n",
    "    dl_dx2 = dloss(x2, t)\n",
    "    dl_ds2 = dsigma(s2) * dl_dx2\n",
    "    dl_dx1 = w2.t().mv(dl_ds2)\n",
    "    dl_ds1 = dsigma(s1) * dl_dx1\n",
    "\n",
    "    dl_dw2.add_(dl_ds2.view(-1, 1).mm(x1.view(1, -1)))\n",
    "    dl_db2.add_(dl_ds2)\n",
    "    dl_dw1.add_(dl_ds1.view(-1, 1).mm(x0.view(1, -1)))\n",
    "    dl_db1.add_(dl_ds1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: \n",
    "\n",
    "Claire incompréhension, relire le cours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Training the network\n",
    "\n",
    "Write the code to train and test a MLP with one hidden layer of 50 units. This network should have an input dimension of 784, which is the dimension of the MNIST training set, and an output dimension of 10, which is the number of classes.\n",
    "You code should:\n",
    "\n",
    "1. Load the data using the provided prologue.load ̇data function, with one-hot label vectors and normalized inputs. Multiply the target label vectors by ζ = 0.9 (so that they are strictly in the value range of tanh).\n",
    "2. Create the four weight and bias tensors, and fill them with random values sampled according to N(0, ε) with ε = 1e − 6.\n",
    "3. Create the four tensors to sum up the gradients on individual samples, with respect to the weights and biases.\n",
    "4. Perform 1, 000 gradient steps with a step size η equal to 0.1 divided by the number of training samples.\n",
    "\n",
    "Each of these steps requires to reset to zero the tensors for summing up the gradients, and doing a forward and a backward pass for each training example.\n",
    "\n",
    "Compute and print the training loss, training error and test error after every step using the class of maximum response as the predicted one.\n",
    "\n",
    "Hint: My solution is 1987 character long and achieves 3.6% training error and 15.70% test error with 50 hidden units. It takes 1min40s to finish on a Intel i7 with no GPU, using the default small sets of prologue.load ̇data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Using MNIST\n",
      "** Reduce the data-set (use --full for the full thing)\n",
      "** Use 1000 train and 1000 test samples\n"
     ]
    }
   ],
   "source": [
    "train_input, train_target, test_input, test_target = prologue.load_data()\n",
    "train_target *= 0.9\n",
    "test_target*= 0.9\n",
    "n_classes = 10\n",
    "hidden_dimension = 50\n",
    "input_dimension = 784\n",
    "n_train = train_input.size(0)\n",
    "n_test = test_input.size(0)\n",
    "\n",
    "epsilon = (10**(-6))\n",
    "w1 = Tensor(hidden_dimension, train_input.size(1)).normal_(0, epsilon)\n",
    "b1 = Tensor(hidden_dimension).normal_(0, epsilon)\n",
    "w2 = Tensor(n_classes, hidden_dimension).normal_(0, epsilon)\n",
    "b2 = Tensor(n_classes).normal_(0, epsilon)\n",
    "\n",
    "dl_dw1 = Tensor(w1.size())\n",
    "dl_dw2 = Tensor(w2.size())\n",
    "dl_db1 = Tensor(b1.size())\n",
    "dl_db2 = Tensor(b2.size())\n",
    "\n",
    "eta = 0.1/n_train\n",
    "\n",
    "for i in range(1000):\n",
    "    acc_loss = 0\n",
    "    nb_train_errors = 0\n",
    "\n",
    "    dl_dw1.zero_()\n",
    "    dl_db1.zero_()\n",
    "    dl_dw2.zero_()\n",
    "    dl_db2.zero_()\n",
    "    \n",
    "    for j in range(n_train):\n",
    "        x, s1, x1, s2, x2 = forward_pass(b1=b1, b2=b2, w1=w1, w2=w2, x=train_input[j])\n",
    "        backward_pass(w1, b1, w2, b2,\n",
    "                  train_target[j].float(),\n",
    "                  x, s1, x1, s2, x2,\n",
    "                  dl_dw1, dl_db1, dl_dw2, dl_db2)\n",
    "    \n",
    "    w1 = w1 - eta * dl_dw1\n",
    "    b1 = b1 - eta * dl_db1\n",
    "    w2 = w2 - eta * dl_dw2\n",
    "    b2 = b2 - eta * dl_db2\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 7.9368e-07, -2.9087e-07, -1.9056e-07,  ..., -4.5025e-09,\n",
      "          5.2184e-07,  1.2222e-07],\n",
      "        [ 1.6297e-07, -8.9330e-07, -2.4524e-07,  ..., -2.8647e-06,\n",
      "          7.7882e-07,  1.0929e-06],\n",
      "        [ 1.3245e-06, -5.0951e-07, -2.6161e-07,  ...,  8.5209e-07,\n",
      "          1.3358e-06,  3.9310e-07],\n",
      "        ...,\n",
      "        [-5.2221e-07,  2.6156e-06, -2.7214e-07,  ...,  1.5129e-07,\n",
      "          8.7381e-07, -1.8542e-07],\n",
      "        [ 7.0642e-07, -1.5696e-06,  1.2612e-06,  ...,  1.4513e-06,\n",
      "          5.7798e-07,  1.4351e-06],\n",
      "        [-8.2720e-07,  1.1239e-06,  8.2591e-07,  ...,  1.5426e-06,\n",
      "          6.9170e-07, -5.1783e-07]])\n",
      "tensor([[ 6.2110e-07,  4.6845e-07,  2.0607e-06, -6.8884e-07, -8.4178e-07,\n",
      "          8.1810e-08,  2.0056e-06,  1.1767e-06, -3.0302e-07, -1.0057e-06,\n",
      "          3.2718e-07, -5.1883e-07,  1.6384e-07,  3.6668e-07, -2.5713e-07,\n",
      "          4.6792e-08,  1.0478e-06, -1.2150e-06, -1.2761e-06, -2.8307e-07,\n",
      "          6.8178e-07, -5.6880e-07,  6.6020e-08,  3.8623e-07,  7.7206e-07,\n",
      "          1.7860e-06, -1.3285e-06,  9.3651e-07, -1.1376e-06,  1.6314e-06,\n",
      "         -2.0386e-06,  1.7140e-06, -1.7827e-06,  3.2235e-07,  2.9070e-07,\n",
      "          5.1605e-07, -1.3344e-06,  7.4156e-07, -7.5032e-07,  9.4377e-07,\n",
      "         -6.0495e-07, -7.1475e-07, -2.1533e-06, -1.2430e-06,  4.4691e-08,\n",
      "          4.4090e-07,  7.2562e-07,  2.2190e-06,  1.1459e-06,  9.4516e-07],\n",
      "        [ 1.5335e-06,  6.2484e-07,  2.9596e-07,  1.4754e-06, -1.0739e-07,\n",
      "          5.8765e-07, -2.0374e-06,  2.0757e-06, -4.2093e-07,  1.3291e-06,\n",
      "          1.2419e-06, -3.8599e-07, -5.7860e-07, -9.2016e-07,  9.1414e-07,\n",
      "          1.4157e-06,  4.6823e-08,  1.4842e-06,  1.0430e-06, -4.5778e-07,\n",
      "         -7.0589e-07, -9.4032e-07,  9.1061e-07,  1.1736e-06,  1.0495e-06,\n",
      "          4.5381e-07,  7.9702e-07,  4.0099e-07,  4.1254e-07, -2.0613e-07,\n",
      "         -5.0075e-07,  5.3582e-08,  1.1142e-06, -6.5314e-07,  3.5964e-07,\n",
      "          1.4777e-06,  9.8689e-07,  3.3118e-07,  1.2717e-06, -6.3467e-07,\n",
      "         -9.9280e-07, -2.9576e-07, -6.8213e-07, -4.1900e-07, -2.7501e-07,\n",
      "         -2.7366e-07, -2.0264e-06, -2.7187e-07, -4.8797e-07,  3.9811e-07],\n",
      "        [-5.7923e-07,  3.4351e-07,  3.9481e-07,  1.2799e-07,  6.5802e-07,\n",
      "         -1.4597e-10,  1.6424e-06, -2.0927e-06, -8.4208e-07,  1.2147e-07,\n",
      "         -3.0418e-07,  1.8643e-07,  1.1709e-06, -6.7425e-08, -1.9155e-07,\n",
      "         -4.5461e-07,  1.0237e-06,  8.1529e-07, -2.6822e-06, -3.0739e-07,\n",
      "          1.9357e-06, -4.6566e-07, -1.2755e-06,  9.8613e-07, -1.3650e-07,\n",
      "         -4.4331e-07, -1.0407e-06,  4.6322e-07,  1.2862e-06, -2.0097e-06,\n",
      "          1.7276e-06,  1.9276e-06,  1.8717e-06,  3.4780e-08, -9.3116e-07,\n",
      "         -7.5976e-07,  6.8826e-07,  9.2932e-09, -6.1589e-07,  1.3379e-06,\n",
      "         -1.0719e-08,  5.5901e-08, -4.0553e-07,  2.3905e-07,  1.8858e-07,\n",
      "          9.2432e-07,  7.5203e-07, -3.5347e-07,  1.1339e-06, -1.8844e-06],\n",
      "        [ 7.3243e-07, -2.7202e-07,  6.1213e-07, -3.2935e-06,  9.7236e-07,\n",
      "          1.5449e-06,  3.7690e-07,  1.0925e-06,  2.5068e-07, -6.4934e-07,\n",
      "          1.3450e-06,  1.8878e-07, -2.8179e-06, -8.8346e-07, -3.7728e-07,\n",
      "          2.5053e-07,  1.0295e-06,  1.4643e-08, -6.9794e-07, -1.6021e-06,\n",
      "         -8.7045e-07,  6.8360e-07, -1.4743e-06, -1.5998e-06,  7.3491e-07,\n",
      "          1.2532e-06, -1.3343e-06,  1.7049e-06, -6.3538e-07,  7.0149e-07,\n",
      "          6.9651e-08,  1.2077e-06,  3.9079e-07,  7.1972e-07,  2.2978e-07,\n",
      "          6.5639e-07,  5.5616e-07, -8.8994e-07, -1.1889e-08, -5.0806e-08,\n",
      "          1.3836e-07,  1.3481e-06,  1.3646e-06, -1.1177e-06,  1.6974e-06,\n",
      "         -3.8951e-08,  6.1891e-07, -1.7882e-07, -1.0210e-06, -1.5387e-06],\n",
      "        [ 5.7914e-07, -3.9368e-07,  5.8055e-09, -9.6386e-07,  1.2708e-07,\n",
      "         -1.0613e-07, -5.0048e-07, -3.1617e-07, -1.7705e-06,  1.3902e-06,\n",
      "          7.6271e-08, -7.9531e-07, -6.7148e-07, -7.9819e-07,  9.6989e-07,\n",
      "         -1.6154e-06, -1.0474e-06, -2.9287e-07,  5.5004e-07,  1.7462e-07,\n",
      "         -1.2702e-06, -5.2827e-07, -2.7644e-07, -6.0569e-08,  1.2500e-06,\n",
      "          1.0085e-06, -8.0832e-07, -1.6386e-06, -7.1765e-07,  8.8124e-07,\n",
      "          1.4466e-06, -8.9449e-07,  6.0667e-09, -8.0436e-07,  3.4409e-07,\n",
      "          7.6250e-07,  4.6030e-07,  2.2537e-08, -4.2204e-07, -4.9734e-07,\n",
      "          2.0945e-06,  5.3614e-07, -1.0268e-06,  1.0725e-06,  2.2026e-06,\n",
      "          1.0954e-08, -1.3893e-06,  7.7721e-08,  4.0980e-09,  5.2356e-07],\n",
      "        [ 7.0345e-07,  1.5264e-06,  6.0343e-07,  5.5440e-07,  1.1819e-06,\n",
      "          1.5104e-06, -4.4594e-07, -6.6413e-08, -9.1338e-07,  9.1050e-07,\n",
      "         -3.4347e-08,  4.4587e-08, -1.9268e-08,  1.3890e-06,  1.0039e-06,\n",
      "          1.1652e-07, -6.5391e-07, -2.9528e-07,  1.2123e-06, -1.7821e-06,\n",
      "         -8.9781e-07,  5.4091e-07,  2.4385e-06,  5.0366e-07, -3.8283e-07,\n",
      "          1.4044e-07,  1.1557e-06,  1.3415e-06,  2.3223e-06,  1.0071e-06,\n",
      "         -1.1963e-06,  1.9746e-06, -1.5787e-06,  1.5162e-06,  1.0521e-06,\n",
      "          5.7402e-07, -8.5771e-07,  1.8865e-07,  2.6362e-07,  1.2366e-06,\n",
      "          1.1564e-06, -5.9048e-07,  7.8345e-07,  2.1112e-07, -9.7113e-07,\n",
      "         -1.2049e-06,  7.7019e-08, -2.6156e-07, -3.9218e-07,  2.6555e-07],\n",
      "        [-4.0872e-07, -4.9725e-07, -7.6941e-07,  8.4163e-07, -1.2402e-06,\n",
      "          9.4713e-07, -2.1874e-06, -1.1191e-07, -1.1385e-06, -8.8442e-07,\n",
      "         -3.1522e-07,  5.2386e-07, -1.2914e-06,  7.0200e-07, -3.6161e-07,\n",
      "         -4.2376e-07, -4.1753e-07, -1.8005e-06, -1.6410e-07,  1.9455e-07,\n",
      "          2.6533e-07, -1.0359e-06,  5.0670e-08, -4.5218e-09, -8.6217e-07,\n",
      "         -1.0514e-07,  6.3011e-07, -1.7818e-06, -1.5754e-06,  5.1532e-08,\n",
      "         -5.5277e-07,  1.6198e-07,  1.1710e-06, -8.7120e-08,  6.5262e-07,\n",
      "          2.5577e-06,  3.8905e-07, -6.5830e-07, -9.0602e-07,  5.3181e-07,\n",
      "         -1.0717e-06,  6.8626e-07, -9.1443e-07, -1.0208e-06, -8.8447e-07,\n",
      "         -8.2238e-07,  8.5198e-08,  9.7758e-07,  5.2261e-07, -1.2516e-06],\n",
      "        [-9.5827e-07, -4.3247e-07, -6.7728e-07, -3.8690e-07, -5.5348e-07,\n",
      "         -1.8696e-06, -1.9532e-06, -6.1698e-07, -2.1993e-07,  1.0834e-06,\n",
      "         -3.9130e-07, -2.4413e-07,  4.6129e-07, -2.8342e-07, -1.9665e-06,\n",
      "          4.9569e-07,  2.8708e-07,  1.8256e-06,  1.3098e-06, -6.0744e-08,\n",
      "         -3.6670e-07, -2.4520e-07,  1.5957e-06, -5.4140e-07,  1.2673e-06,\n",
      "         -2.7163e-07, -1.0260e-06, -8.2937e-07,  5.3561e-07,  3.0232e-07,\n",
      "          3.0430e-08,  9.0457e-07,  1.2124e-06,  2.1493e-07, -2.6038e-07,\n",
      "          1.9347e-07, -5.4791e-07, -1.2632e-06, -4.0710e-07,  1.8734e-07,\n",
      "          9.0814e-07, -1.5164e-06, -3.1187e-07, -1.4071e-06,  9.3787e-07,\n",
      "         -1.1384e-06, -1.8671e-07,  2.9187e-07,  5.4345e-09,  2.6275e-07],\n",
      "        [-1.1219e-06,  3.0893e-07, -2.0139e-07,  3.0419e-07, -2.5903e-08,\n",
      "         -5.6053e-07,  3.8596e-07, -2.9524e-07, -1.0140e-06,  8.0352e-07,\n",
      "         -2.5229e-06,  3.2035e-07, -2.3993e-06, -1.6933e-06, -6.4631e-08,\n",
      "         -3.6438e-08, -9.8092e-07,  9.1913e-07, -3.6523e-07, -6.9327e-07,\n",
      "         -2.6076e-07,  9.1990e-07, -6.7484e-07,  1.3808e-07, -6.1941e-08,\n",
      "         -8.9317e-07,  5.6207e-07, -5.3940e-07, -3.9963e-08,  1.5649e-06,\n",
      "          1.7319e-06, -1.8994e-07,  5.0649e-07,  4.6886e-07,  2.0529e-06,\n",
      "          2.3567e-07,  2.8229e-07,  4.1350e-07, -7.4072e-07, -4.3269e-07,\n",
      "         -1.3092e-06,  1.5828e-07, -1.4577e-06,  9.2207e-07, -1.9494e-06,\n",
      "         -3.6382e-07,  1.2174e-06, -6.5480e-07, -1.5303e-06, -2.2832e-06],\n",
      "        [-1.6479e-06, -2.3350e-07, -9.1233e-08, -1.4284e-06,  1.2454e-06,\n",
      "         -5.4790e-07,  5.2608e-08,  2.6083e-07, -3.3693e-07, -9.4573e-07,\n",
      "          1.5873e-07, -3.3822e-07, -9.1766e-07, -1.1044e-06,  8.6171e-08,\n",
      "         -5.6166e-07,  5.7994e-07, -5.6363e-08,  2.1908e-07,  9.5459e-07,\n",
      "          2.8727e-07, -5.3699e-07, -1.0711e-06,  4.5596e-07,  1.1652e-06,\n",
      "         -4.6652e-07, -1.4869e-06,  4.0107e-07, -7.6063e-08, -3.7330e-07,\n",
      "         -1.5741e-06,  1.7251e-06,  3.7495e-07, -2.0155e-06, -7.9132e-07,\n",
      "         -2.8362e-07,  5.0163e-07, -1.1650e-07,  1.2305e-06, -7.2588e-07,\n",
      "          6.9647e-07, -1.0628e-06,  7.8020e-07, -5.7420e-07, -1.9216e-06,\n",
      "         -4.0765e-07,  1.3622e-06,  1.1277e-06,  5.0288e-07, -1.3867e-06]])\n",
      "tensor([ 1.1916e-06,  3.8218e-07,  7.9351e-07,  5.5912e-07, -2.0595e-07,\n",
      "         2.2451e-07, -9.5970e-08,  1.6687e-07, -4.3665e-07, -4.4019e-07,\n",
      "        -3.0778e-07, -7.7177e-08, -1.0767e-06,  4.6080e-07,  4.2830e-07,\n",
      "        -1.4448e-06,  2.6644e-07,  1.2697e-06,  1.4277e-07, -6.9675e-07,\n",
      "         7.8282e-07,  8.8846e-07, -1.2559e-06,  7.2689e-07, -6.4522e-08,\n",
      "        -8.2515e-07, -3.3633e-07,  7.9214e-08, -5.2192e-07, -3.4537e-08,\n",
      "         8.5097e-07, -4.6222e-08, -1.8148e-07,  1.8424e-06, -5.5831e-07,\n",
      "         9.6799e-07, -9.8791e-07,  7.4634e-07,  3.0705e-07,  4.0293e-07,\n",
      "         8.0673e-07,  4.4845e-07,  6.9833e-07,  1.8131e-07, -4.4222e-07,\n",
      "        -6.4915e-07,  1.7461e-06,  5.3501e-07,  4.6137e-07, -1.2596e-06])\n",
      "tensor([-2.0111e-10,  3.1791e-09,  4.0275e-09, -1.1465e-09, -7.8750e-09,\n",
      "         9.3479e-09,  1.0377e-08,  2.0879e-09, -8.7810e-09,  6.4282e-09])\n"
     ]
    }
   ],
   "source": [
    "print(w1)\n",
    "print(w2)\n",
    "print(b1)\n",
    "print(b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Using MNIST\n",
      "** Reduce the data-set (use --full for the full thing)\n",
      "** Use 1000 train and 1000 test samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amarchand/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:41: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/Users/amarchand/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:66: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    }
   ],
   "source": [
    "train_input, train_target, test_input, test_target = prologue.load_data(one_hot_labels = True,\n",
    "                                                                        normalize = True)\n",
    "\n",
    "nb_classes = train_target.size(1)\n",
    "nb_train_samples = train_input.size(0)\n",
    "\n",
    "zeta = 0.90\n",
    "\n",
    "train_input = train_input * zeta\n",
    "test_input = test_input * zeta\n",
    "\n",
    "nb_hidden = 50\n",
    "eta = 1e-1 / nb_train_samples\n",
    "epsilon = 1e-6\n",
    "\n",
    "w1 = Tensor(nb_hidden, train_input.size(1)).normal_(0, epsilon)\n",
    "b1 = Tensor(nb_hidden).normal_(0, epsilon)\n",
    "w2 = Tensor(nb_classes, nb_hidden).normal_(0, epsilon)\n",
    "b2 = Tensor(nb_classes).normal_(0, epsilon)\n",
    "\n",
    "dl_dw1 = Tensor(w1.size())\n",
    "dl_db1 = Tensor(b1.size())\n",
    "dl_dw2 = Tensor(w2.size())\n",
    "dl_db2 = Tensor(b2.size())\n",
    "\n",
    "for k in range(0, 1000):\n",
    "\n",
    "    # Back-prop\n",
    "\n",
    "    acc_loss = 0\n",
    "    nb_train_errors = 0\n",
    "\n",
    "    dl_dw1.zero_()\n",
    "    dl_db1.zero_()\n",
    "    dl_dw2.zero_()\n",
    "    dl_db2.zero_()\n",
    "\n",
    "    for n in range(0, nb_train_samples):\n",
    "        x0, s1, x1, s2, x2 = forward_pass(w1, b1, w2, b2, train_input[n])\n",
    "\n",
    "        pred = x2.max(0)[1][0]\n",
    "        if train_target[n, pred] < 0: \n",
    "            nb_train_errors = nb_train_errors + 1\n",
    "            \n",
    "        acc_loss = acc_loss + loss(x2, train_target[n])\n",
    "\n",
    "        backward_pass(w1, b1, w2, b2,\n",
    "                      train_target[n],\n",
    "                      x0, s1, x1, s2, x2,\n",
    "                      dl_dw1, dl_db1, dl_dw2, dl_db2)\n",
    "\n",
    "    # Gradient step\n",
    "\n",
    "    w1 = w1 - eta * dl_dw1\n",
    "    b1 = b1 - eta * dl_db1\n",
    "    w2 = w2 - eta * dl_dw2\n",
    "    b2 = b2 - eta * dl_db2\n",
    "\n",
    "    # Test error\n",
    "\n",
    "    nb_test_errors = 0\n",
    "\n",
    "    for n in range(0, test_input.size(0)):\n",
    "        _, _, _, _, x2 = forward_pass(w1, b1, w2, b2, test_input[n])\n",
    "\n",
    "        pred = x2.max(0)[1][0]\n",
    "        if test_target[n, pred] < 0: nb_test_errors = nb_test_errors + 1\n",
    "    if i%100 == 0:\n",
    "        print('{:d} acc_train_loss {:.02f} acc_train_error {:.02f}% test_error {:.02f}%'\n",
    "              .format(k,\n",
    "                      acc_loss,\n",
    "                      (100 * nb_train_errors) / train_input.size(0),\n",
    "                      (100 * nb_test_errors) / test_input.size(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0062, -0.0062, -0.0062,  ..., -0.0062, -0.0062, -0.0062],\n",
      "        [-0.0030, -0.0030, -0.0030,  ..., -0.0030, -0.0030, -0.0030],\n",
      "        [-0.0021, -0.0021, -0.0021,  ..., -0.0021, -0.0021, -0.0021],\n",
      "        ...,\n",
      "        [ 0.0007,  0.0008,  0.0008,  ...,  0.0008,  0.0008,  0.0008],\n",
      "        [ 0.0095,  0.0095,  0.0095,  ...,  0.0095,  0.0095,  0.0095],\n",
      "        [ 0.0090,  0.0090,  0.0090,  ...,  0.0090,  0.0090,  0.0090]])\n",
      "tensor([[-0.0100,  0.0813,  0.1226,  0.0825,  0.1825, -0.0622, -0.1460,  0.4874,\n",
      "          0.0037, -0.0068,  0.3498,  0.3570, -0.4793,  0.1693, -0.0469,  0.0732,\n",
      "         -0.4761,  0.0552, -0.0450, -0.2526,  0.1625,  0.2181, -0.0513, -0.2099,\n",
      "         -0.0087, -0.0637,  0.0406,  0.1108,  0.1112, -0.2052, -0.1830,  0.2817,\n",
      "          0.2019,  0.0639, -0.1164, -0.1447,  0.1226,  0.1929,  0.0133, -0.0197,\n",
      "          0.1552, -0.0178, -0.1760,  0.0067, -0.1987, -0.0190, -0.2251,  0.1664,\n",
      "          0.2019,  0.2134],\n",
      "        [-0.3586, -0.2734, -0.2013, -0.2553, -0.0488,  0.1689,  0.1008,  0.0676,\n",
      "          0.3040, -0.3050,  0.0641, -0.0884, -0.0712, -0.1552,  0.2667, -0.2336,\n",
      "         -0.0705, -0.2024,  0.3544,  0.1321, -0.1364,  0.0836,  0.1606, -0.0839,\n",
      "         -0.3076,  0.2063, -0.2528, -0.2531, -0.2354, -0.0841,  0.0573, -0.1302,\n",
      "          0.0843, -0.2858,  0.2762,  0.2174, -0.2677,  0.0870,  0.2212, -0.1263,\n",
      "         -0.0476, -0.1144,  0.0755, -0.1582, -0.0846,  0.3785, -0.0554, -0.0960,\n",
      "          0.0962,  0.0836],\n",
      "        [ 0.1598, -0.2298, -0.3908, -0.4080,  0.2773,  0.1106,  0.3196, -0.0230,\n",
      "         -0.2644,  0.1368, -0.0464,  0.1710,  0.0311,  0.4504, -0.2103,  0.0107,\n",
      "          0.0442, -0.0256, -0.0950,  0.3045,  0.2398,  0.1808,  0.0923, -0.1888,\n",
      "          0.1619,  0.0715, -0.1887, -0.6249,  0.5383, -0.1949,  0.3286,  0.0994,\n",
      "          0.1978,  0.2386,  0.4852, -0.0003,  0.4518,  0.2176, -0.0419, -0.0766,\n",
      "          0.0520, -0.1042, -0.3783, -0.0618, -0.2036,  0.0334,  0.0741,  0.1029,\n",
      "          0.2434,  0.1858],\n",
      "        [ 0.1080,  0.1030, -0.5358,  0.1799,  0.1007, -0.4440,  0.5816,  0.0137,\n",
      "         -0.0361,  0.0007,  0.1004, -0.1238, -0.0203,  0.3743,  0.1808,  0.1095,\n",
      "         -0.0263,  0.0155, -0.2598, -0.4235,  0.3461,  0.1952, -0.0548, -0.2034,\n",
      "         -0.0170, -0.1760,  0.0805,  0.0510, -0.4558, -0.2100,  0.3473, -0.2515,\n",
      "          0.2131, -0.4861, -0.0994,  0.3097, -0.2538,  0.2353,  0.0364, -0.1827,\n",
      "         -0.0039, -0.1979, -0.1233, -0.1218, -0.2196, -0.0514, -0.1443,  0.1670,\n",
      "          0.2634,  0.2001],\n",
      "        [-0.0090,  0.0788,  0.0115,  0.1467, -0.2786, -0.0879,  0.0203,  0.0453,\n",
      "          0.0470, -0.1693,  0.1239, -0.1380, -0.0764, -0.1686,  0.0470,  0.1899,\n",
      "         -0.0840,  0.1230, -0.0391,  0.1720, -0.4286,  0.1612, -0.5613, -0.1670,\n",
      "         -0.0979, -0.0265,  0.0893, -0.0040,  0.3477, -0.1701, -0.1174,  0.1343,\n",
      "          0.1723, -0.1549, -0.0487, -0.1785,  0.3349,  0.1789,  0.2655, -0.4653,\n",
      "         -0.4956, -0.5109,  0.2222, -0.4730, -0.1745,  0.0134, -0.2756, -0.3934,\n",
      "          0.1893,  0.1650],\n",
      "        [-0.3375,  0.1092,  0.4576,  0.1988,  0.4809, -0.0163, -0.5536, -0.2150,\n",
      "          0.3502, -0.3677, -0.2631, -0.2212,  0.1965,  0.4469, -0.2540, -0.0408,\n",
      "          0.1945, -0.1228, -0.0554, -0.2690,  0.2062,  0.2206, -0.0254, -0.2264,\n",
      "         -0.4092,  0.2598,  0.0607,  0.1941,  0.0077, -0.2299, -0.2249, -0.2939,\n",
      "          0.2317,  0.3826, -0.0957, -0.0621, -0.2620,  0.2407,  0.3424, -0.2996,\n",
      "          0.1506, -0.2369, -0.4265, -0.2828, -0.2345,  0.0416,  0.3629,  0.3178,\n",
      "          0.2550,  0.2240],\n",
      "        [ 0.0317, -0.2788,  0.0249, -0.3317, -0.4675, -0.1012, -0.2347,  0.0850,\n",
      "          0.2584,  0.0078, -0.0729, -0.1510, -0.0619, -0.4470, -0.0455,  0.0831,\n",
      "         -0.0451,  0.0931,  0.0053,  0.0954,  0.3714,  0.2310, -0.1195, -0.2417,\n",
      "         -0.0961, -0.0738, -0.3551,  0.3010,  0.2179, -0.2505, -0.2378, -0.0837,\n",
      "          0.2547,  0.0270,  0.0238, -0.1404,  0.3369,  0.2813, -0.0041,  0.0473,\n",
      "          0.4461,  0.0819,  0.6761,  0.1077, -0.2629,  0.0168,  0.1770,  0.1020,\n",
      "          0.3094,  0.2376],\n",
      "        [ 0.0385,  0.1381,  0.0897,  0.1172, -0.2408,  0.2736,  0.0164,  0.0750,\n",
      "         -0.0936,  0.1114,  0.1382,  0.6509, -0.0758, -0.0675,  0.0944, -0.4585,\n",
      "         -0.0864, -0.5156, -0.1737, -0.3391, -0.1792,  0.1700,  0.3151, -0.1705,\n",
      "          0.0989,  0.3390,  0.1389,  0.0841, -0.3141, -0.1703, -0.2087,  0.5006,\n",
      "          0.1708,  0.0292, -0.1290,  0.4011, -0.1672,  0.1740, -0.0471, -0.1264,\n",
      "         -0.3681, -0.1550,  0.0677, -0.0521, -0.1712, -0.1517, -0.1008, -0.2449,\n",
      "          0.1867,  0.1703],\n",
      "        [ 0.1027,  0.2341,  0.1651,  0.1522,  0.3807,  0.5602,  0.3810,  0.1660,\n",
      "         -0.1066,  0.1442, -0.0547, -0.1951, -0.1303, -0.4867,  0.4309, -0.1587,\n",
      "         -0.1043,  0.1849,  0.1169,  0.5734, -0.6078,  0.2582,  0.0501, -0.2649,\n",
      "          0.1156, -0.1762,  0.2530,  0.1197,  0.2268, -0.2709, -0.1030, -0.1746,\n",
      "          0.2736, -0.1646, -0.2108, -0.1596, -0.1381,  0.2982, -0.0733,  0.0139,\n",
      "          0.3712,  0.0172, -0.2674,  0.0575, -0.2802, -0.1677,  0.1724,  0.2633,\n",
      "          0.3348,  0.2624],\n",
      "        [-0.0645,  0.0539,  0.2009,  0.0609, -0.2733, -0.2391, -0.1728,  0.2828,\n",
      "         -0.0983,  0.0231,  0.0148, -0.3859, -0.2379, -0.1627, -0.1468,  0.2397,\n",
      "         -0.1879,  0.3369, -0.0776,  0.3375, -0.1093,  0.2516,  0.8636, -0.2585,\n",
      "         -0.0331, -0.4124,  0.0548,  0.0710, -0.3953, -0.2645,  0.6108, -0.3586,\n",
      "          0.2674,  0.1704,  0.0011, -0.3044, -0.1550,  0.2892, -0.0819,  0.1838,\n",
      "         -0.3078,  0.2372,  0.3107,  0.2578, -0.2736,  0.0227,  0.2453, -0.2149,\n",
      "          0.3190,  0.2558]])\n",
      "tensor([ 0.0163,  0.0079,  0.0056,  0.0102,  0.0003, -0.0173, -0.0090, -0.0221,\n",
      "        -0.0136,  0.0096, -0.0116, -0.0107,  0.0217,  0.0171, -0.0243,  0.0093,\n",
      "         0.0216,  0.0065, -0.0065, -0.0168,  0.0125, -0.0237, -0.0178,  0.0238,\n",
      "         0.0102,  0.0005,  0.0068,  0.0039,  0.0049,  0.0239, -0.0087,  0.0001,\n",
      "        -0.0239,  0.0201, -0.0098,  0.0027,  0.0066, -0.0242, -0.0084,  0.0168,\n",
      "        -0.0051,  0.0178, -0.0072,  0.0110,  0.0240, -0.0078, -0.0013, -0.0020,\n",
      "        -0.0252, -0.0238])\n",
      "tensor([-0.7981, -0.6381, -0.8550, -0.8833, -0.7596, -0.8532, -0.9162, -0.7459,\n",
      "        -0.9812, -0.9380])\n"
     ]
    }
   ],
   "source": [
    "print(w1)\n",
    "print(w2)\n",
    "print(b1)\n",
    "print(b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
