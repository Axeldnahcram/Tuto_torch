{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The objective of this session is to illustrate on a 2D synthetic toy data-set how poorly a naive weight initialization procedure performs when a network has multiple layers of different sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Toy data-set\n",
    "\n",
    "Write a function\n",
    "\n",
    "    generate disc set(nb)\n",
    "    \n",
    "that returns a pair torch.Tensor , torch.LongTensor of dimensions respectively nb ×2 and nb ,\n",
    "corresponding to the input and target of a toy data-set where the input is uniformly distributed in [−1, 1] × [−1, 1] and the label is 1 inside the disc of radius 􏰒 (2/π)\\**0.5 and 0 outside.\n",
    "\n",
    "\n",
    "Create a train and test set of 1, 000 samples, and normalize their mean and variance to 0 and 1. A simple sanity check is to ensure that the two classes are balanced.\n",
    "\n",
    "Hint: My version of generate disc set is 172 characters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Training and test\n",
    "\n",
    "Write functions\n",
    "    \n",
    "    train model(model, train input, train target)\n",
    "    compute nb errors(model, data input, data target)\n",
    "    \n",
    "The first should train the model with cross-entropy and 250 epochs of standard sgd with η = 0.1, and mini-batches of size 100.\n",
    "\n",
    "The second should also use mini-batches, and return an integer.\n",
    "\n",
    "Hint: My versions of train model and compute nb errors are respectively 512 and 457 characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Models\n",
    "\n",
    "Write\n",
    "    \n",
    "    create shallow model()\n",
    "\n",
    "that returns a mlp with 2 input units, a single hidden layer of size 128, and 2 output units, and\n",
    "\n",
    "    create deep model()\n",
    "    \n",
    "that returns a mlp with 2 input units, hidden layers of sizes respectively 4, 8, 16, 32, 64, 128, and 2 output units.\n",
    "\n",
    "Hint: You can use the nn.Sequential container to make things simpler. \n",
    "My versions of these two functions are respectively 132 and 355 characters long.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Benchmarking\n",
    "\n",
    "Compute and print the train and test errors of these two models when they are initialized either with the default pytorch rule, or with a normal distribution of standard deviation 10−3 , 10−2 , 10−1 , 1, and 10.\n",
    "\n",
    "\n",
    "The error rate with the shallow network for any initialization should be around 1.5%. It should be around 3% with the deep network using the default rule, and around 50% most of the time with the other initializations.\n",
    "\n",
    "Hint: My version is 562 characters long."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
